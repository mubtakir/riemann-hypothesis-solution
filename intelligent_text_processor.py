#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ
Intelligent Scientific Text Processing System

Ø§Ù„Ù…Ø¤Ù„Ù: Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
Ø§Ù„ØªØ§Ø±ÙŠØ®: 2025-07-26
Ø§Ù„ØºØ±Ø¶: Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
"""

import re
import json
import hashlib
from typing import List, Dict, Tuple, Set
from dataclasses import dataclass
from collections import defaultdict
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class ScientificIdea:
    """ÙØ¦Ø© Ù„ØªÙ…Ø«ÙŠÙ„ ÙÙƒØ±Ø© Ø¹Ù„Ù…ÙŠØ©"""
    id: str
    title: str
    content: str
    category: str
    date: str
    equations: List[str]
    keywords: List[str]
    similarity_hash: str
    importance_score: float

class ArabicScientificTextProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self):
        self.ideas_database = {}
        self.categories = {
            'Ù†Ø¸Ø±ÙŠØ©': ['Ù†Ø¸Ø±ÙŠØ©', 'Ù…Ø¨Ø±Ù‡Ù†Ø©', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø¨Ø¯Ø£'],
            'Ù…Ø¹Ø§Ø¯Ù„Ø©': ['Ù…Ø¹Ø§Ø¯Ù„Ø©', 'ØµÙŠØºØ©', 'ØªØ¹Ø¨ÙŠØ± Ø±ÙŠØ§Ø¶ÙŠ'],
            'Ø¨Ø±Ù‡Ø§Ù†': ['Ø¨Ø±Ù‡Ø§Ù†', 'Ø¥Ø«Ø¨Ø§Øª', 'Ø¯Ù„ÙŠÙ„', 'Ø§Ø³ØªÙ†ØªØ§Ø¬'],
            'ØªØ·Ø¨ÙŠÙ‚': ['ØªØ·Ø¨ÙŠÙ‚', 'Ù…Ø«Ø§Ù„', 'Ø­Ø§Ù„Ø© Ø®Ø§ØµØ©'],
            'ØªØ­Ù„ÙŠÙ„': ['ØªØ­Ù„ÙŠÙ„', 'Ø¯Ø±Ø§Ø³Ø©', 'ÙØ­Øµ', 'ØªÙ‚ÙŠÙŠÙ…'],
            'Ø®Ù„Ø§ØµØ©': ['Ø®Ù„Ø§ØµØ©', 'Ù†ØªÙŠØ¬Ø©', 'Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù†Ù‡Ø§Ø¦ÙŠ']
        }
        self.equation_patterns = [
            r'\$\$.*?\$\$',  # LaTeX display equations
            r'\$.*?\$',      # LaTeX inline equations
            r'\\begin\{.*?\}.*?\\end\{.*?\}',  # LaTeX environments
            r'[A-Za-z]+\s*=\s*[^ØŒ\.]+',  # Simple equations
        ]
        
    def extract_equations(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ"""
        equations = []
        for pattern in self.equation_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            equations.extend(matches)
        return equations
    
    def extract_keywords(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©"""
        scientific_keywords = [
            'Ù‡Ø§Ù…Ù„ØªÙˆÙ†ÙŠ', 'Ø·ÙŠÙ', 'Ù‚ÙŠÙ…Ø© Ø°Ø§ØªÙŠØ©', 'Ø¯Ø§Ù„Ø© Ø²ÙŠØªØ§', 'ÙØ±Ø¶ÙŠØ© Ø±ÙŠÙ…Ø§Ù†',
            'Ù…Ø´ØºÙ„ Ù‡ÙŠØ±Ù…ÙŠØªÙŠ', 'ØªÙ†Ø§Ø¸Ø±', 'Ø§Ù†Ø­Ù„Ø§Ù„', 'ÙƒÙ…ÙˆÙ…ÙŠ', 'Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒØ§',
            'Ø£Ø¹Ø¯Ø§Ø¯ Ø£ÙˆÙ„ÙŠØ©', 'ØªÙˆØ²ÙŠØ¹', 'ØªØ­Ù„ÙŠÙ„', 'Ø¨Ø±Ù‡Ø§Ù†', 'Ù†Ø¸Ø±ÙŠØ©'
        ]
        
        found_keywords = []
        text_lower = text.lower()
        for keyword in scientific_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def calculate_similarity_hash(self, content: str) -> str:
        """Ø­Ø³Ø§Ø¨ Ù‡Ø§Ø´ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ù„Ù…Ø­ØªÙˆÙ‰"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        cleaned = re.sub(r'[^\w\s]', '', content)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip().lower()
        return hashlib.md5(cleaned.encode('utf-8')).hexdigest()
    
    def calculate_importance_score(self, idea: ScientificIdea) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ÙÙƒØ±Ø©"""
        score = 0.0
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª
        score += len(idea.equations) * 2.0
        
        # Ù†Ù‚Ø§Ø· Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        score += len(idea.keywords) * 1.5
        
        # Ù†Ù‚Ø§Ø· Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙØµÙ„ Ø£Ù‡Ù…)
        score += min(len(idea.content) / 100, 5.0)
        
        # Ù†Ù‚Ø§Ø· Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        important_categories = ['Ù†Ø¸Ø±ÙŠØ©', 'Ø¨Ø±Ù‡Ø§Ù†', 'Ù…Ø¹Ø§Ø¯Ù„Ø©']
        if idea.category in important_categories:
            score += 3.0
            
        return score
    
    def categorize_idea(self, content: str) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙƒØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        content_lower = content.lower()
        
        for category, keywords in self.categories.items():
            for keyword in keywords:
                if keyword in content_lower:
                    return category
        
        return 'Ø¹Ø§Ù…'
    
    def parse_idea_block(self, block: str) -> ScientificIdea:
        """ØªØ­Ù„ÙŠÙ„ ÙƒØªÙ„Ø© Ù†Øµ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙƒØ±Ø© Ø¹Ù„Ù…ÙŠØ©"""
        lines = block.strip().split('\n')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† (Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø£ÙˆÙ„ Ø¹Ø§Ø¯Ø©)
        title = lines[0].strip('#* ').strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®
        date_match = re.search(r'Ø§Ù„ØªØ§Ø±ÙŠØ®.*?(\d{4}-\d{2}-\d{2})', block)
        date = date_match.group(1) if date_match else "2025-07-26"
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content = '\n'.join(lines[1:]).strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª
        equations = self.extract_equations(content)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = self.extract_keywords(content)
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„ÙÙƒØ±Ø©
        category = self.categorize_idea(content)
        
        # Ø­Ø³Ø§Ø¨ Ù‡Ø§Ø´ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity_hash = self.calculate_similarity_hash(content)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯
        idea_id = f"idea_{len(self.ideas_database) + 1:03d}"
        
        idea = ScientificIdea(
            id=idea_id,
            title=title,
            content=content,
            category=category,
            date=date,
            equations=equations,
            keywords=keywords,
            similarity_hash=similarity_hash,
            importance_score=0.0
        )
        
        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        idea.importance_score = self.calculate_importance_score(idea)
        
        return idea
    
    def detect_duplicates(self, new_idea: ScientificIdea, threshold: float = 0.8) -> List[str]:
        """ÙƒØ´Ù Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        duplicates = []
        
        for existing_id, existing_idea in self.ideas_database.items():
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù‡Ø§Ø´
            if new_idea.similarity_hash == existing_idea.similarity_hash:
                duplicates.append(existing_id)
                continue
            
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            similarity = self.calculate_content_similarity(new_idea.content, existing_idea.content)
            if similarity > threshold:
                duplicates.append(existing_id)
        
        return duplicates
    
    def calculate_content_similarity(self, content1: str, content2: str) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø³ÙŠØ·Ø©"""
        words1 = set(content1.lower().split())
        words2 = set(content2.lower().split())
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        if len(union) == 0:
            return 0.0
        
        return len(intersection) / len(union)
    
    def process_text_file(self, file_path: str) -> Dict[str, List[ScientificIdea]]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù†ØµÙŠ ÙƒØ§Ù…Ù„"""
        logger.info(f"Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
            return {}
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒØªÙ„ Ø§Ù„Ø£ÙÙƒØ§Ø±
        idea_blocks = self.split_into_idea_blocks(content)
        
        results = {
            'new_ideas': [],
            'duplicates': [],
            'updated_ideas': []
        }
        
        for block in idea_blocks:
            if not block.strip():
                continue
                
            try:
                idea = self.parse_idea_block(block)
                duplicates = self.detect_duplicates(idea)
                
                if duplicates:
                    # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
                    best_duplicate = self.find_best_version(idea, duplicates)
                    if best_duplicate:
                        results['duplicates'].append({
                            'new': idea,
                            'existing': best_duplicate,
                            'action': 'keep_existing' if best_duplicate.importance_score > idea.importance_score else 'replace'
                        })
                else:
                    # ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©
                    self.ideas_database[idea.id] = idea
                    results['new_ideas'].append(idea)
                    
            except Exception as e:
                logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØªÙ„Ø© Ø§Ù„ÙÙƒØ±Ø©: {e}")
                continue
        
        logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results['new_ideas'])} ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©")
        logger.info(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results['duplicates'])} ØªÙƒØ±Ø§Ø±")
        
        return results
    
    def split_into_idea_blocks(self, content: str) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒØªÙ„ Ø§Ù„Ø£ÙÙƒØ§Ø±"""
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø£ÙÙƒØ§Ø±
        idea_patterns = [
            r'###\s+.*?\(\d+\)',  # ### Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙÙƒØ±Ø© (Ø±Ù‚Ù…)
            r'\*\*.*?\(\d+\)\*\*',  # **Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙÙƒØ±Ø© (Ø±Ù‚Ù…)**
            r'\d+\.\s+\*\*.*?\*\*',  # 1. **Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙÙƒØ±Ø©**
        ]
        
        blocks = []
        current_block = ""
        
        lines = content.split('\n')
        
        for line in lines:
            is_new_idea = False
            
            for pattern in idea_patterns:
                if re.match(pattern, line.strip()):
                    is_new_idea = True
                    break
            
            if is_new_idea and current_block.strip():
                blocks.append(current_block.strip())
                current_block = line + '\n'
            else:
                current_block += line + '\n'
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        if current_block.strip():
            blocks.append(current_block.strip())
        
        return blocks
    
    def find_best_version(self, new_idea: ScientificIdea, duplicate_ids: List[str]) -> ScientificIdea:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        best_idea = new_idea
        best_score = new_idea.importance_score
        
        for dup_id in duplicate_ids:
            existing_idea = self.ideas_database[dup_id]
            if existing_idea.importance_score > best_score:
                best_idea = existing_idea
                best_score = existing_idea.importance_score
        
        return best_idea
    
    def generate_organized_output(self, output_file: str):
        """Ø¥Ù†ØªØ§Ø¬ Ù…Ù„Ù Ù…Ù†Ø¸Ù… Ù„Ù„Ø£ÙÙƒØ§Ø±"""
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø© ÙˆØ§Ù„Ø£Ù‡Ù…ÙŠØ©
        categorized_ideas = defaultdict(list)
        
        for idea in self.ideas_database.values():
            categorized_ideas[idea.category].append(idea)
        
        # ØªØ±ØªÙŠØ¨ ÙƒÙ„ ÙØ¦Ø© Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        for category in categorized_ideas:
            categorized_ideas[category].sort(key=lambda x: x.importance_score, reverse=True)
        
        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù…
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ù…Ù†Ø¸Ù…Ø©\n")
            f.write(f"## Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø±: {len(self.ideas_database)}\n\n")
            
            for category, ideas in categorized_ideas.items():
                f.write(f"## ÙØ¦Ø©: {category} ({len(ideas)} ÙÙƒØ±Ø©)\n\n")
                
                for idea in ideas:
                    f.write(f"### {idea.title} ({idea.id})\n")
                    f.write(f"**Ø§Ù„ØªØ§Ø±ÙŠØ®:** {idea.date}\n")
                    f.write(f"**Ø§Ù„ÙØ¦Ø©:** {idea.category}\n")
                    f.write(f"**Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©:** {idea.importance_score:.2f}\n")
                    
                    if idea.keywords:
                        f.write(f"**Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©:** {', '.join(idea.keywords)}\n")
                    
                    if idea.equations:
                        f.write(f"**Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª:** {len(idea.equations)} Ù…Ø¹Ø§Ø¯Ù„Ø©\n")
                    
                    f.write(f"\n{idea.content}\n\n")
                    f.write("---\n\n")
        
        logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù…: {output_file}")
    
    def save_database(self, file_path: str):
        """Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙÙƒØ§Ø±"""
        data = {}
        for idea_id, idea in self.ideas_database.items():
            data[idea_id] = {
                'id': idea.id,
                'title': idea.title,
                'content': idea.content,
                'category': idea.category,
                'date': idea.date,
                'equations': idea.equations,
                'keywords': idea.keywords,
                'similarity_hash': idea.similarity_hash,
                'importance_score': idea.importance_score
            }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {file_path}")
    
    def load_database(self, file_path: str):
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙÙƒØ§Ø±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for idea_id, idea_data in data.items():
                idea = ScientificIdea(**idea_data)
                self.ideas_database[idea_id] = idea
            
            logger.info(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.ideas_database)} ÙÙƒØ±Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
    processor = ArabicScientificTextProcessor()
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ø§Ù„Ø£ÙÙƒØ§Ø±
    results = processor.process_text_file('riemann_research_ideas.md')
    
    # Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ±
    print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results['new_ideas'])} ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©")
    print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results['duplicates'])} ØªÙƒØ±Ø§Ø±")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    processor.generate_organized_output('organized_ideas.md')
    processor.save_database('ideas_database.json')

class AdvancedTextAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""

    def __init__(self):
        self.processor = ArabicScientificTextProcessor()

    def install_dependencies(self):
        """ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
        import subprocess
        import sys

        packages = [
            'scikit-learn',
            'numpy',
            'matplotlib',
            'seaborn',
            'wordcloud',
            'arabic-reshaper',
            'python-bidi'
        ]

        for package in packages:
            try:
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])
                logger.info(f"ØªÙ… ØªØ«Ø¨ÙŠØª {package} Ø¨Ù†Ø¬Ø§Ø­")
            except Exception as e:
                logger.error(f"ÙØ´Ù„ ÙÙŠ ØªØ«Ø¨ÙŠØª {package}: {e}")

    def advanced_similarity_analysis(self, ideas: List[ScientificIdea]) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
            texts = [idea.content for idea in ideas]

            # Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© TF-IDF
            vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=None,  # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                ngram_range=(1, 2)
            )

            tfidf_matrix = vectorizer.fit_transform(texts)

            # Ø­Ø³Ø§Ø¨ Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            similarity_matrix = cosine_similarity(tfidf_matrix)

            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
            similar_pairs = []
            threshold = 0.7

            for i in range(len(ideas)):
                for j in range(i + 1, len(ideas)):
                    similarity = similarity_matrix[i][j]
                    if similarity > threshold:
                        similar_pairs.append({
                            'idea1': ideas[i],
                            'idea2': ideas[j],
                            'similarity': similarity
                        })

            return {
                'similarity_matrix': similarity_matrix,
                'similar_pairs': similar_pairs,
                'vectorizer': vectorizer
            }

        except ImportError:
            logger.warning("Ù…ÙƒØªØ¨Ø© scikit-learn ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©. ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØªÙ‡Ø§ Ø£ÙˆÙ„Ø§Ù‹.")
            return {}

    def generate_word_cloud(self, ideas: List[ScientificIdea], output_file: str):
        """Ø¥Ù†ØªØ§Ø¬ Ø³Ø­Ø§Ø¨Ø© ÙƒÙ„Ù…Ø§Øª Ù„Ù„Ø£ÙÙƒØ§Ø±"""
        try:
            from wordcloud import WordCloud
            import matplotlib.pyplot as plt
            import arabic_reshaper
            from bidi.algorithm import get_display

            # Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ
            all_text = ' '.join([idea.content for idea in ideas])

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            reshaped_text = arabic_reshaper.reshape(all_text)
            bidi_text = get_display(reshaped_text)

            # Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            wordcloud = WordCloud(
                width=1200,
                height=800,
                background_color='white',
                max_words=100,
                font_path='NotoSansArabic-Regular.ttf',  # ÙŠØ­ØªØ§Ø¬ Ø®Ø· Ø¹Ø±Ø¨ÙŠ
                colormap='viridis'
            ).generate(bidi_text)

            # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
            plt.figure(figsize=(15, 10))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('Ø³Ø­Ø§Ø¨Ø© ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø¹Ù„Ù…ÙŠØ©', fontsize=16)
            plt.savefig(output_file, dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {output_file}")

        except ImportError:
            logger.warning("Ù…ÙƒØªØ¨Ø§Øª Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")

    def generate_statistics_report(self, ideas: List[ScientificIdea]) -> Dict:
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠ Ø´Ø§Ù…Ù„"""
        stats = {
            'total_ideas': len(ideas),
            'categories': {},
            'equations_count': 0,
            'keywords_frequency': {},
            'importance_distribution': [],
            'monthly_distribution': {}
        }

        for idea in ideas:
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙØ¦Ø§Øª
            if idea.category not in stats['categories']:
                stats['categories'][idea.category] = 0
            stats['categories'][idea.category] += 1

            # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª
            stats['equations_count'] += len(idea.equations)

            # ØªÙƒØ±Ø§Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            for keyword in idea.keywords:
                if keyword not in stats['keywords_frequency']:
                    stats['keywords_frequency'][keyword] = 0
                stats['keywords_frequency'][keyword] += 1

            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            stats['importance_distribution'].append(idea.importance_score)

            # Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø´Ù‡Ø±ÙŠ
            month = idea.date[:7]  # YYYY-MM
            if month not in stats['monthly_distribution']:
                stats['monthly_distribution'][month] = 0
            stats['monthly_distribution'][month] += 1

        return stats

    def create_visual_dashboard(self, ideas: List[ScientificIdea], output_dir: str):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… Ø¨ØµØ±ÙŠØ©"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            import numpy as np
            import os

            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
            os.makedirs(output_dir, exist_ok=True)

            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            plt.rcParams['font.family'] = ['Arial Unicode MS', 'Tahoma', 'DejaVu Sans']

            stats = self.generate_statistics_report(ideas)

            # 1. Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„ÙØ¦Ø§Øª
            plt.figure(figsize=(12, 8))
            categories = list(stats['categories'].keys())
            counts = list(stats['categories'].values())

            plt.subplot(2, 2, 1)
            plt.pie(counts, labels=categories, autopct='%1.1f%%', startangle=90)
            plt.title('ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©')

            # 2. ØªÙˆØ²ÙŠØ¹ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            plt.subplot(2, 2, 2)
            plt.hist(stats['importance_distribution'], bins=20, alpha=0.7, color='skyblue')
            plt.xlabel('Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©')
            plt.ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙÙƒØ§Ø±')
            plt.title('ØªÙˆØ²ÙŠØ¹ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø£Ù‡Ù…ÙŠØ©')

            # 3. Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            plt.subplot(2, 2, 3)
            top_keywords = sorted(stats['keywords_frequency'].items(),
                                key=lambda x: x[1], reverse=True)[:10]
            keywords, frequencies = zip(*top_keywords)

            plt.barh(range(len(keywords)), frequencies)
            plt.yticks(range(len(keywords)), keywords)
            plt.xlabel('Ø§Ù„ØªÙƒØ±Ø§Ø±')
            plt.title('Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©')

            # 4. Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø²Ù…Ù†ÙŠ
            plt.subplot(2, 2, 4)
            months = list(stats['monthly_distribution'].keys())
            month_counts = list(stats['monthly_distribution'].values())

            plt.plot(months, month_counts, marker='o')
            plt.xlabel('Ø§Ù„Ø´Ù‡Ø±')
            plt.ylabel('Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙÙƒØ§Ø±')
            plt.title('Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ø£ÙÙƒØ§Ø±')
            plt.xticks(rotation=45)

            plt.tight_layout()
            plt.savefig(f"{output_dir}/dashboard.png", dpi=300, bbox_inches='tight')
            plt.close()

            logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…: {output_dir}/dashboard.png")

        except ImportError:
            logger.warning("Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")

    def export_to_formats(self, ideas: List[ScientificIdea], base_filename: str):
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ø£ÙÙƒØ§Ø± Ù„ØµÙŠØº Ù…ØªØ¹Ø¯Ø¯Ø©"""

        # ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ JSON
        json_data = []
        for idea in ideas:
            json_data.append({
                'id': idea.id,
                'title': idea.title,
                'content': idea.content,
                'category': idea.category,
                'date': idea.date,
                'equations_count': len(idea.equations),
                'keywords': idea.keywords,
                'importance_score': idea.importance_score
            })

        with open(f"{base_filename}.json", 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

        # ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ CSV
        try:
            import csv

            with open(f"{base_filename}.csv", 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['ID', 'Title', 'Category', 'Date', 'Keywords', 'Importance', 'Content_Length'])

                for idea in ideas:
                    writer.writerow([
                        idea.id,
                        idea.title,
                        idea.category,
                        idea.date,
                        '; '.join(idea.keywords),
                        idea.importance_score,
                        len(idea.content)
                    ])

            logger.info(f"ØªÙ… Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰: {base_filename}.json, {base_filename}.csv")

        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØµØ¯ÙŠØ±: {e}")

def run_complete_analysis():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
    print("ğŸš€ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ")
    print("=" * 50)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„Ù„
    analyzer = AdvancedTextAnalyzer()

    # ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    # analyzer.install_dependencies()

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    files_to_process = [
        'riemann_research_ideas.md',
        'conversation_archive.txt'
    ]

    all_ideas = []

    for file_path in files_to_process:
        if os.path.exists(file_path):
            print(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {file_path}")
            results = analyzer.processor.process_text_file(file_path)
            all_ideas.extend(results['new_ideas'])

            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            print(f"  âœ… Ø£ÙÙƒØ§Ø± Ø¬Ø¯ÙŠØ¯Ø©: {len(results['new_ideas'])}")
            print(f"  ğŸ”„ ØªÙƒØ±Ø§Ø±Ø§Øª: {len(results['duplicates'])}")
        else:
            print(f"âš ï¸  Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")

    if all_ideas:
        print(f"\nğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(all_ideas)}")

        # Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
        print("ğŸ“ˆ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±...")

        # ØªÙ‚Ø±ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠ
        stats = analyzer.generate_statistics_report(all_ideas)
        print(f"  ğŸ“‹ Ø§Ù„ÙØ¦Ø§Øª: {len(stats['categories'])}")
        print(f"  ğŸ”¢ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: {stats['equations_count']}")
        print(f"  ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {len(stats['keywords_frequency'])}")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity_results = analyzer.advanced_similarity_analysis(all_ideas)
        if similarity_results:
            print(f"  ğŸ” Ø£Ø²ÙˆØ§Ø¬ Ù…ØªØ´Ø§Ø¨Ù‡Ø©: {len(similarity_results['similar_pairs'])}")

        # Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª
        analyzer.processor.generate_organized_output('organized_ideas_final.md')
        analyzer.export_to_formats(all_ideas, 'ideas_export')

        # Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¨ØµØ±ÙŠØ©
        analyzer.create_visual_dashboard(all_ideas, 'visual_reports')

        # Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        analyzer.generate_word_cloud(all_ideas, 'wordcloud.png')

        print("\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        print("ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙÙ†ØªØ¬Ø©:")
        print("  - organized_ideas_final.md")
        print("  - ideas_export.json")
        print("  - ideas_export.csv")
        print("  - visual_reports/dashboard.png")
        print("  - wordcloud.png")

    else:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙÙƒØ§Ø± Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")

if __name__ == "__main__":
    import os
    run_complete_analysis()
