#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ØªØ´ØºÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ
Quick Run for Intelligent Scientific Text Processing System

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
python run_text_analysis.py

Ø£Ùˆ Ù…Ø¹ Ø®ÙŠØ§Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©:
python run_text_analysis.py --advanced --export-all --visualize
"""

import argparse
import sys
import os
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø³Ø§Ø±
sys.path.append(str(Path(__file__).parent))

try:
    from intelligent_text_processor import AdvancedTextAnalyzer, ArabicScientificTextProcessor
except ImportError:
    print("âŒ Ø®Ø·Ø£: Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ØµÙˆØµ")
    print("ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„Ù intelligent_text_processor.py ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù„Ø¯")
    sys.exit(1)

def check_dependencies():
    """ÙØ­Øµ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©"""
    required_basic = ['numpy', 'matplotlib', 'json', 're', 'hashlib']
    optional = ['scikit-learn', 'wordcloud', 'seaborn', 'arabic_reshaper']
    
    missing_basic = []
    missing_optional = []
    
    for lib in required_basic:
        try:
            __import__(lib)
        except ImportError:
            missing_basic.append(lib)
    
    for lib in optional:
        try:
            __import__(lib)
        except ImportError:
            missing_optional.append(lib)
    
    if missing_basic:
        print(f"âŒ Ù…ÙƒØªØ¨Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø©: {', '.join(missing_basic)}")
        print("ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: pip install " + ' '.join(missing_basic))
        return False
    
    if missing_optional:
        print(f"âš ï¸  Ù…ÙƒØªØ¨Ø§Øª Ø§Ø®ØªÙŠØ§Ø±ÙŠØ© Ù…ÙÙ‚ÙˆØ¯Ø©: {', '.join(missing_optional)}")
        print("Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØªÙ‡Ø§: pip install " + ' '.join(missing_optional))
    
    return True

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(
        description='Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  python run_text_analysis.py                    # ØªØ´ØºÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ
  python run_text_analysis.py --advanced         # ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…
  python run_text_analysis.py --export-all       # ØªØµØ¯ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙŠØº
  python run_text_analysis.py --visualize        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
  python run_text_analysis.py --file custom.md   # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù…Ø­Ø¯Ø¯
        """
    )
    
    parser.add_argument('--file', '-f', 
                       help='Ù…Ù„Ù Ù…Ø­Ø¯Ø¯ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ: riemann_research_ideas.md)')
    parser.add_argument('--advanced', '-a', action='store_true',
                       help='ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ')
    parser.add_argument('--export-all', '-e', action='store_true',
                       help='ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙŠØº (JSON, CSV, MD)')
    parser.add_argument('--visualize', '-v', action='store_true',
                       help='Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ÙˆØ³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª')
    parser.add_argument('--output-dir', '-o', default='analysis_output',
                       help='Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: analysis_output)')
    parser.add_argument('--similarity-threshold', '-s', type=float, default=0.7,
                       help='Ø¹ØªØ¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„ÙƒØ´Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 0.7)')
    parser.add_argument('--check-deps', action='store_true',
                       help='ÙØ­Øµ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙ‚Ø·')
    
    args = parser.parse_args()
    
    # ÙØ­Øµ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
    if args.check_deps:
        check_dependencies()
        return
    
    if not check_dependencies():
        print("ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø£ÙˆÙ„Ø§Ù‹")
        return
    
    print("ğŸš€ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ù„Ù…ÙŠØ© Ø§Ù„Ø°ÙƒÙŠ")
    print("=" * 60)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    if args.file:
        files_to_process = [args.file]
    else:
        files_to_process = [
            'riemann_research_ideas.md',
            'riemann_hypothesis_solution_book.md',
            'conversation_archive.txt'
        ]
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
    if args.advanced:
        analyzer = AdvancedTextAnalyzer()
        processor = analyzer.processor
    else:
        processor = ArabicScientificTextProcessor()
        analyzer = None
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
    all_ideas = []
    total_new = 0
    total_duplicates = 0
    
    for file_path in files_to_process:
        if not os.path.exists(file_path):
            print(f"âš ï¸  Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
            continue
        
        print(f"\nğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {file_path}")
        print("-" * 40)
        
        try:
            results = processor.process_text_file(file_path)
            
            new_ideas = results.get('new_ideas', [])
            duplicates = results.get('duplicates', [])
            
            all_ideas.extend(new_ideas)
            total_new += len(new_ideas)
            total_duplicates += len(duplicates)
            
            print(f"  âœ… Ø£ÙÙƒØ§Ø± Ø¬Ø¯ÙŠØ¯Ø©: {len(new_ideas)}")
            print(f"  ğŸ”„ ØªÙƒØ±Ø§Ø±Ø§Øª: {len(duplicates)}")
            
            # Ø¹Ø±Ø¶ Ø£Ù‡Ù… Ø§Ù„Ø£ÙÙƒØ§Ø±
            if new_ideas:
                top_ideas = sorted(new_ideas, key=lambda x: x.importance_score, reverse=True)[:3]
                print("  ğŸŒŸ Ø£Ù‡Ù… Ø§Ù„Ø£ÙÙƒØ§Ø±:")
                for i, idea in enumerate(top_ideas, 1):
                    print(f"    {i}. {idea.title} (Ø£Ù‡Ù…ÙŠØ©: {idea.importance_score:.2f})")
            
        except Exception as e:
            print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {e}")
            continue
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
    print(f"\nğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©:")
    print(f"  ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø±: {len(all_ideas)}")
    print(f"  âœ¨ Ø£ÙÙƒØ§Ø± Ø¬Ø¯ÙŠØ¯Ø©: {total_new}")
    print(f"  ğŸ”„ ØªÙƒØ±Ø§Ø±Ø§Øª: {total_duplicates}")
    
    if not all_ideas:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙÙƒØ§Ø± Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        return
    
    # Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    print(f"\nğŸ“ˆ Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±...")
    
    # Ù…Ù„Ù Ù…Ù†Ø¸Ù…
    organized_file = output_dir / 'organized_ideas.md'
    processor.generate_organized_output(str(organized_file))
    print(f"  ğŸ“‹ Ù…Ù„Ù Ù…Ù†Ø¸Ù…: {organized_file}")
    
    # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª
    db_file = output_dir / 'ideas_database.json'
    processor.save_database(str(db_file))
    print(f"  ğŸ’¾ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {db_file}")
    
    # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
    if args.advanced and analyzer:
        print(f"\nğŸ§  Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity_results = analyzer.advanced_similarity_analysis(all_ideas)
        if similarity_results:
            similar_pairs = similarity_results.get('similar_pairs', [])
            print(f"  ğŸ” Ø£Ø²ÙˆØ§Ø¬ Ù…ØªØ´Ø§Ø¨Ù‡Ø©: {len(similar_pairs)}")
            
            if similar_pairs:
                print("  ğŸ“‹ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡:")
                for pair in similar_pairs[:3]:
                    print(f"    - {pair['idea1'].title} â†” {pair['idea2'].title} "
                          f"(ØªØ´Ø§Ø¨Ù‡: {pair['similarity']:.2f})")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        stats = analyzer.generate_statistics_report(all_ideas)
        print(f"  ğŸ“Š Ø§Ù„ÙØ¦Ø§Øª: {len(stats['categories'])}")
        print(f"  ğŸ”¢ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: {stats['equations_count']}")
        print(f"  ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {len(stats['keywords_frequency'])}")
        
        # Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        if stats['keywords_frequency']:
            top_keywords = sorted(stats['keywords_frequency'].items(), 
                                key=lambda x: x[1], reverse=True)[:5]
            print("  ğŸ·ï¸  Ø£Ù‡Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©:")
            for keyword, count in top_keywords:
                print(f"    - {keyword}: {count}")
    
    # Ø§Ù„ØªØµØ¯ÙŠØ±
    if args.export_all:
        print(f"\nğŸ“¤ ØªØµØ¯ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        export_base = output_dir / 'ideas_export'
        
        if analyzer:
            analyzer.export_to_formats(all_ideas, str(export_base))
        else:
            # ØªØµØ¯ÙŠØ± Ø£Ø³Ø§Ø³ÙŠ
            import json
            json_data = [
                {
                    'id': idea.id,
                    'title': idea.title,
                    'category': idea.category,
                    'importance': idea.importance_score
                }
                for idea in all_ideas
            ]
            
            with open(f"{export_base}.json", 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"  ğŸ’¾ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØµØ¯ÙŠØ±: {export_base}.*")
    
    # Ø§Ù„ØªØµÙˆØ±
    if args.visualize and analyzer:
        print(f"\nğŸ¨ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØµÙˆØ±Ø§Øª...")
        
        # Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…
        visual_dir = output_dir / 'visualizations'
        analyzer.create_visual_dashboard(all_ideas, str(visual_dir))
        print(f"  ğŸ“Š Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ…: {visual_dir}/dashboard.png")
        
        # Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        wordcloud_file = output_dir / 'wordcloud.png'
        analyzer.generate_word_cloud(all_ideas, str(wordcloud_file))
        print(f"  â˜ï¸  Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {wordcloud_file}")
    
    print(f"\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
    print(f"ğŸ“ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: {output_dir}")
    
    # Ù†ØµØ§Ø¦Ø­ Ù„Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©
    print(f"\nğŸ’¡ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:")
    print(f"  1. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù…: {organized_file}")
    print(f"  2. Ø§Ø³ØªØ®Ø¯Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¨Ø­Ø«: {db_file}")
    if args.visualize:
        print(f"  3. Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØªØµÙˆØ±Ø§Øª ÙÙŠ: {output_dir}/visualizations/")
    if not args.advanced:
        print(f"  4. Ø¬Ø±Ø¨ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: --advanced")
    if not args.export_all:
        print(f"  5. ØµØ¯Ù‘Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: --export-all")

if __name__ == "__main__":
    main()
