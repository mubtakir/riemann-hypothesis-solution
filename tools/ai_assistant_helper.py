#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ÙØ±Ø² ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ
AI Assistant Helper for Text Sorting and Analysis

Ø§Ù„ØºØ±Ø¶: Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ:
1. Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ Ù…Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø£Ø³Ø·Ø±
2. Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙÙƒØ§Ø± Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
3. ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙƒØ±Ø±
4. ØªØ³Ù‡ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙØ±Ø² ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ…
"""

import re
import json
from typing import List, Dict, Tuple, Set
from difflib import SequenceMatcher
from collections import defaultdict
import hashlib

class AIAssistantHelper:
    """Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ÙØ±Ø² Ø§Ù„Ù†ØµÙˆØµ"""
    
    def __init__(self, context_words=10):
        self.file_content = ""
        self.lines = []
        self.similarity_threshold = 0.7
        self.context_words = context_words  # Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚
        
    def load_file(self, file_path: str) -> bool:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù„Ù„ØªØ­Ù„ÙŠÙ„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                self.file_content = f.read()
                self.lines = self.file_content.split('\n')
            print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {file_path}")
            print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø±: {len(self.lines)}")
            return True
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: {e}")
            return False
    
    def find_similar_lines(self, threshold: float = 0.7) -> List[Dict]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù…Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø£Ø³Ø·Ø±"""
        print(f"\nğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª (Ø¹ØªØ¨Ø©: {threshold})...")
        
        similar_groups = []
        processed_lines = set()
        
        for i, line1 in enumerate(self.lines):
            if i in processed_lines or len(line1.strip()) < 10:
                continue
                
            similar_lines = [(i + 1, line1)]  # Ø±Ù‚Ù… Ø§Ù„Ø³Ø·Ø± (1-based)
            
            for j, line2 in enumerate(self.lines[i+1:], i+1):
                if len(line2.strip()) < 10:
                    continue
                    
                similarity = self._calculate_similarity(line1, line2)
                if similarity >= threshold:
                    similar_lines.append((j + 1, line2))
                    processed_lines.add(j)
            
            if len(similar_lines) > 1:
                similar_groups.append({
                    'similarity_score': max([self._calculate_similarity(similar_lines[0][1], line[1]) 
                                           for line in similar_lines[1:]]),
                    'lines': similar_lines
                })
                processed_lines.add(i)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similar_groups.sort(key=lambda x: x['similarity_score'], reverse=True)
        
        print(f"ğŸ“‹ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(similar_groups)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªØ´Ø§Ø¨Ù‡Ø©")
        return similar_groups
    
    def search_concept(self, concept: str, context_lines: int = 2) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ÙÙ‡ÙˆÙ… Ù…Ø­Ø¯Ø¯ ÙÙŠ Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        print(f"\nğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: '{concept}'")
        
        results = []
        concept_lower = concept.lower()
        
        for i, line in enumerate(self.lines):
            if concept_lower in line.lower():
                # Ø¬Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚
                start_line = max(0, i - context_lines)
                end_line = min(len(self.lines), i + context_lines + 1)
                
                context = []
                for j in range(start_line, end_line):
                    marker = ">>> " if j == i else "    "
                    context.append(f"{marker}Ø§Ù„Ø³Ø·Ø± {j+1}: {self.lines[j]}")
                
                results.append({
                    'line_number': i + 1,
                    'content': line,
                    'context': context
                })
        
        print(f"ğŸ“‹ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø©")
        return results
    
    def find_duplicates_exact(self) -> List[Dict]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹"""
        print(f"\nğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©...")
        
        line_hash_map = defaultdict(list)
        
        for i, line in enumerate(self.lines):
            cleaned_line = re.sub(r'\s+', ' ', line.strip().lower())
            if len(cleaned_line) > 10:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„Ù‚ØµÙŠØ±Ø©
                line_hash = hashlib.md5(cleaned_line.encode()).hexdigest()
                line_hash_map[line_hash].append((i + 1, line))
        
        duplicates = []
        for hash_key, line_list in line_hash_map.items():
            if len(line_list) > 1:
                duplicates.append({
                    'hash': hash_key,
                    'count': len(line_list),
                    'lines': line_list
                })
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        duplicates.sort(key=lambda x: x['count'], reverse=True)
        
        print(f"ğŸ“‹ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(duplicates)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø©")
        return duplicates
    
    def find_ideas_by_pattern(self, pattern: str) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£ÙÙƒØ§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø· regex"""
        print(f"\nğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù†Ù…Ø·: '{pattern}'")
        
        results = []
        try:
            regex = re.compile(pattern, re.IGNORECASE | re.MULTILINE)
            
            for i, line in enumerate(self.lines):
                matches = regex.finditer(line)
                for match in matches:
                    results.append({
                        'line_number': i + 1,
                        'content': line,
                        'match': match.group(),
                        'start_pos': match.start(),
                        'end_pos': match.end()
                    })
        
        except re.error as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ù…Ø·: {e}")
            return []
        
        print(f"ğŸ“‹ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø©")
        return results
    
    def analyze_section_structure(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†"""
        print(f"\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Øµ...")
        
        sections = {
            'main_headers': [],      # ###
            'sub_headers': [],       # **text**
            'numbered_items': [],    # 1. 2. 3.
            'bullet_points': [],     # - * â€¢
            'equations': [],         # $$ Ø£Ùˆ Ù…Ø¹Ø§Ø¯Ù„Ø§Øª
            'dates': []             # ØªÙˆØ§Ø±ÙŠØ®
        }
        
        patterns = {
            'main_headers': r'^#{1,6}\s+(.+)',
            'sub_headers': r'\*\*(.+?)\*\*',
            'numbered_items': r'^\s*\d+\.\s+(.+)',
            'bullet_points': r'^\s*[-*â€¢]\s+(.+)',
            'equations': r'\$\$?.+?\$\$?',
            'dates': r'\d{4}-\d{2}-\d{2}'
        }
        
        for i, line in enumerate(self.lines):
            for section_type, pattern in patterns.items():
                matches = re.finditer(pattern, line)
                for match in matches:
                    sections[section_type].append({
                        'line_number': i + 1,
                        'content': line.strip(),
                        'match': match.group()
                    })
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        for section_type, items in sections.items():
            print(f"  {section_type}: {len(items)} Ø¹Ù†ØµØ±")
        
        return sections
    
    def generate_line_map(self, search_terms: List[str]) -> Dict:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø£Ø³Ø·Ø± Ù„Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©"""
        print(f"\nğŸ—ºï¸ Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ø³Ø·Ø±...")
        
        line_map = {}
        
        for term in search_terms:
            term_lower = term.lower()
            line_map[term] = []
            
            for i, line in enumerate(self.lines):
                if term_lower in line.lower():
                    line_map[term].append({
                        'line_number': i + 1,
                        'content': line.strip()[:100] + "..." if len(line.strip()) > 100 else line.strip()
                    })
        
        return line_map
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù†ØµÙŠÙ†"""
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ
        clean1 = re.sub(r'[^\w\s]', '', text1.lower().strip())
        clean2 = re.sub(r'[^\w\s]', '', text2.lower().strip())

        if not clean1 or not clean2:
            return 0.0

        return SequenceMatcher(None, clean1, clean2).ratio()

    def _count_words_between_lines(self, line1: int, line2: int) -> int:
        """Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¨ÙŠÙ† Ø³Ø·Ø±ÙŠÙ†"""
        start_line = min(line1, line2)
        end_line = max(line1, line2)

        word_count = 0
        for i in range(start_line, end_line + 1):
            if i < len(self.lines):
                words = len(self.lines[i].split())
                word_count += words

        return word_count

    def _is_same_context(self, line1: int, line2: int) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø·Ø±Ø§Ù† ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        word_count = self._count_words_between_lines(line1, line2)
        return word_count <= (self.context_words * 2)  # Ø¶Ø¹Ù Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ù„Ù„Ø£Ù…Ø§Ù†

    def _find_context_boundaries(self, center_line: int) -> Tuple[int, int]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø­ÙˆÙ„ Ø³Ø·Ø± Ù…Ø¹ÙŠÙ†"""
        start_line = center_line
        end_line = center_line
        words_before = 0
        words_after = 0

        # Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø®Ù„Ù
        for i in range(center_line - 1, -1, -1):
            if i < len(self.lines):
                line_words = len(self.lines[i].split())
                if words_before + line_words <= self.context_words:
                    words_before += line_words
                    start_line = i
                    # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
                    if self.lines[i].strip().startswith('###'):
                        break
                else:
                    break

        # Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø£Ù…Ø§Ù…
        for i in range(center_line + 1, len(self.lines)):
            line_words = len(self.lines[i].split())
            if words_after + line_words <= self.context_words:
                words_after += line_words
                end_line = i
                # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
                if self.lines[i].strip().startswith('###'):
                    break
            else:
                break

        return start_line, end_line

    def search_concept_with_context(self, concept: str) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ÙÙ‡ÙˆÙ… Ù…Ø¹ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·"""
        print(f"\nğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: '{concept}' Ù…Ø¹ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ù…Ø¹Ø§Ù…Ù„: {self.context_words})")

        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø£ÙˆÙ„Ø§Ù‹
        raw_results = []
        concept_lower = concept.lower()

        for i, line in enumerate(self.lines):
            if concept_lower in line.lower():
                raw_results.append(i)

        if not raw_results:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
            return []

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø©
        grouped_results = []
        processed_lines = set()

        for line_num in raw_results:
            if line_num in processed_lines:
                continue

            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø³ÙŠØ§Ù‚
            start_line, end_line = self._find_context_boundaries(line_num)

            # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚
            context_matches = []
            for other_line in raw_results:
                if start_line <= other_line <= end_line:
                    context_matches.append(other_line)
                    processed_lines.add(other_line)

            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„
            context_text = []
            for i in range(start_line, end_line + 1):
                if i < len(self.lines):
                    marker = ">>> " if i in context_matches else "    "
                    context_text.append(f"{marker}Ø§Ù„Ø³Ø·Ø± {i+1}: {self.lines[i]}")

            grouped_results.append({
                'concept': concept,
                'matches_count': len(context_matches),
                'line_numbers': [l + 1 for l in context_matches],  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ 1-based
                'context_start': start_line + 1,
                'context_end': end_line + 1,
                'context_text': context_text,
                'word_count': self._count_words_between_lines(start_line, end_line)
            })

        print(f"ğŸ“‹ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(grouped_results)} Ø³ÙŠØ§Ù‚ Ù…ØªØ±Ø§Ø¨Ø·")
        return grouped_results

    def analyze_duplicates_with_context(self) -> List[Dict]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø³ÙŠØ§Ù‚"""
        print(f"\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ù…Ø¹Ø§Ù…Ù„: {self.context_words})...")

        duplicates = self.find_duplicates_exact()
        context_analysis = []

        for dup_group in duplicates:
            line_numbers = [line[0] - 1 for line in dup_group['lines']]  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ 0-based

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„ÙƒÙ„ ØªÙƒØ±Ø§Ø±
            contexts = []
            for line_num in line_numbers:
                start_line, end_line = self._find_context_boundaries(line_num)
                context_info = {
                    'line_number': line_num + 1,
                    'context_start': start_line + 1,
                    'context_end': end_line + 1,
                    'word_count': self._count_words_between_lines(start_line, end_line)
                }
                contexts.append(context_info)

            # ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³ÙŠØ§Ù‚
            same_context_groups = []
            processed = set()

            for i, context1 in enumerate(contexts):
                if i in processed:
                    continue

                group = [context1]
                processed.add(i)

                for j, context2 in enumerate(contexts[i+1:], i+1):
                    if j in processed:
                        continue

                    # ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Ø§ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³ÙŠØ§Ù‚
                    if self._is_same_context(context1['line_number'] - 1, context2['line_number'] - 1):
                        group.append(context2)
                        processed.add(j)

                same_context_groups.append(group)

            context_analysis.append({
                'content': dup_group['lines'][0][1],
                'total_occurrences': len(dup_group['lines']),
                'context_groups': same_context_groups,
                'analysis': 'same_context' if len(same_context_groups) == 1 else 'different_contexts'
            })

        return context_analysis

    def find_related_concepts(self, concepts: List[str]) -> Dict:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        print(f"\nğŸ”— Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø©...")

        concept_locations = {}

        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙˆØ§Ù‚Ø¹ ÙƒÙ„ Ù…ÙÙ‡ÙˆÙ…
        for concept in concepts:
            locations = []
            concept_lower = concept.lower()

            for i, line in enumerate(self.lines):
                if concept_lower in line.lower():
                    locations.append(i)

            concept_locations[concept] = locations

        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø©
        related_groups = []

        for concept1, locations1 in concept_locations.items():
            for location1 in locations1:
                related_in_context = [concept1]
                context_start, context_end = self._find_context_boundaries(location1)

                for concept2, locations2 in concept_locations.items():
                    if concept1 == concept2:
                        continue

                    for location2 in locations2:
                        if context_start <= location2 <= context_end:
                            if concept2 not in related_in_context:
                                related_in_context.append(concept2)

                if len(related_in_context) > 1:
                    related_groups.append({
                        'center_line': location1 + 1,
                        'context_range': (context_start + 1, context_end + 1),
                        'related_concepts': related_in_context,
                        'context_text': [f"Ø§Ù„Ø³Ø·Ø± {i+1}: {self.lines[i]}"
                                       for i in range(context_start, context_end + 1)
                                       if i < len(self.lines)]
                    })

        return {
            'concept_locations': {k: [l+1 for l in v] for k, v in concept_locations.items()},
            'related_groups': related_groups
        }

    def remove_ai_responses(self) -> List[Tuple[int, int]]:
        """Ø­Ø°Ù Ø±Ø¯ÙˆØ¯ ÙˆØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        print(f"\nğŸ¤– Ø­Ø°Ù Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...")

        ai_patterns = [
            r'Ù…Ù…ØªØ§Ø²!',
            r'Ø±Ø§Ø¦Ø¹!',
            r'ÙÙƒØ±Ø© Ø¹Ø¨Ù‚Ø±ÙŠØ©',
            r'Ù‡Ø°Ø§ ØªØ·ÙˆÙŠØ± Ù…Ø°Ù‡Ù„',
            r'Ø£ÙÙ‡Ù… ØªÙ…Ø§Ù…Ø§Ù‹',
            r'Ø¯Ø¹Ù†ÙŠ Ø£',
            r'Ø³Ø£Ù‚ÙˆÙ… Ø¨',
            r'Ø§Ù„Ø¢Ù† Ø³Ø£',
            r'Ù…Ù…ÙƒÙ† Ø£Ù† Ø£',
            r'ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£',
            r'âœ….*ØªÙ…',
            r'ğŸ“Š.*Ø§Ù„Ù†ØªØ§Ø¦Ø¬',
            r'ğŸ¯.*Ø§Ù„Ù‡Ø¯Ù',
            r'ğŸ’¡.*ÙÙƒØ±Ø©',
            r'ğŸ”.*Ø§Ù„Ø¨Ø­Ø«',
            r'## ğŸ“Š.*ØªÙ‚Ø±ÙŠØ±',
            r'### âœ….*Ù…Ø§ ØªÙ…',
            r'### ğŸ¯.*Ø§Ù„Ù‡Ø¯Ù'
        ]

        removed_sections = []
        lines_to_remove = set()

        for i, line in enumerate(self.lines):
            line_lower = line.lower().strip()

            # ÙØ­Øµ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            for pattern in ai_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    # ØªØ­Ø¯ÙŠØ¯ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù‚Ø³Ù… Ù„Ù„Ø­Ø°Ù
                    start_line = i
                    end_line = i

                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø³Ù…
                    for j in range(i + 1, min(i + 20, len(self.lines))):
                        next_line = self.lines[j].strip()
                        # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø¯ÙŠØ¯ Ø£Ùˆ ÙÙ‚Ø±Ø© Ø¬Ø¯ÙŠØ¯Ø©
                        if (next_line.startswith('###') and
                            not any(re.search(p, next_line, re.IGNORECASE) for p in ai_patterns)):
                            break
                        if (next_line.startswith('**') and
                            not any(re.search(p, next_line, re.IGNORECASE) for p in ai_patterns)):
                            break
                        end_line = j

                    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø³Ø·Ø± Ù„Ù„Ø­Ø°Ù
                    for k in range(start_line, end_line + 1):
                        lines_to_remove.add(k)

                    removed_sections.append((start_line + 1, end_line + 1))
                    break

        print(f"ğŸ“‹ ØªÙ… ØªØ­Ø¯ÙŠØ¯ {len(removed_sections)} Ù‚Ø³Ù… Ù„Ù„Ø­Ø°Ù")
        return removed_sections

    def extract_solution_method(self, keywords: List[str], method_name: str) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø·Ø±ÙŠÙ‚Ø© Ø­Ù„ Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        print(f"\nğŸ”¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø·Ø±ÙŠÙ‚Ø©: {method_name}")
        print(f"ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {', '.join(keywords)}")

        method_sections = []
        processed_lines = set()

        for keyword in keywords:
            keyword_lower = keyword.lower()

            for i, line in enumerate(self.lines):
                if i in processed_lines:
                    continue

                if keyword_lower in line.lower():
                    # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù‚Ø³Ù…
                    start_line, end_line = self._find_method_boundaries(i, keywords)

                    # Ø¬Ù…Ø¹ Ø§Ù„Ù†Øµ
                    section_text = []
                    for j in range(start_line, end_line + 1):
                        if j < len(self.lines):
                            section_text.append(self.lines[j])
                            processed_lines.add(j)

                    method_sections.append({
                        'start_line': start_line + 1,
                        'end_line': end_line + 1,
                        'trigger_keyword': keyword,
                        'text': '\n'.join(section_text),
                        'word_count': len(' '.join(section_text).split()),
                        'line_count': len(section_text)
                    })

        return {
            'method_name': method_name,
            'keywords': keywords,
            'sections': method_sections,
            'total_sections': len(method_sections)
        }

    def _find_method_boundaries(self, center_line: int, keywords: List[str]) -> Tuple[int, int]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ù„"""
        start_line = center_line
        end_line = center_line

        # Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø®Ù„Ù Ø­ØªÙ‰ Ø¹Ù†ÙˆØ§Ù† Ø£Ùˆ Ø¨Ø¯Ø§ÙŠØ© Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰
        for i in range(center_line - 1, max(0, center_line - 50), -1):
            line = self.lines[i].strip()

            # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            if line.startswith('###') or line.startswith('##'):
                start_line = i + 1
                break

            # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ø·Ø±Ù‚ Ø£Ø®Ø±Ù‰
            if any(kw.lower() in line.lower() for kw in keywords):
                start_line = i
            else:
                start_line = i

        # Ø§Ù„Ø¨Ø­Ø« Ù„Ù„Ø£Ù…Ø§Ù… Ø­ØªÙ‰ Ø¹Ù†ÙˆØ§Ù† Ø£Ùˆ Ø¨Ø¯Ø§ÙŠØ© Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰
        for i in range(center_line + 1, min(len(self.lines), center_line + 50)):
            line = self.lines[i].strip()

            # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            if line.startswith('###') or line.startswith('##'):
                end_line = i - 1
                break

            # Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù†Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ø·Ø±Ù‚ Ø£Ø®Ø±Ù‰
            if any(kw.lower() in line.lower() for kw in keywords):
                end_line = i
            else:
                end_line = i

        return start_line, end_line

    def compare_method_versions(self, sections: List[Dict]) -> Dict:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ù†Ø³Ø® Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø­Ù„"""
        print(f"\nâš–ï¸ Ù…Ù‚Ø§Ø±Ù†Ø© {len(sections)} Ù†Ø³Ø®Ø© Ù…Ù† Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©...")

        if len(sections) <= 1:
            return {'recommendation': 'keep_all', 'sections': sections}

        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©
        scored_sections = []
        for section in sections:
            score = self._calculate_quality_score(section)
            scored_sections.append({**section, 'quality_score': score})

        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø©
        scored_sections.sort(key=lambda x: x['quality_score'], reverse=True)

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity_matrix = []
        for i, sec1 in enumerate(scored_sections):
            row = []
            for j, sec2 in enumerate(scored_sections):
                if i == j:
                    row.append(1.0)
                else:
                    similarity = self._calculate_similarity(sec1['text'], sec2['text'])
                    row.append(similarity)
            similarity_matrix.append(row)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ©
        max_similarity = max(max(row) for row in similarity_matrix if len(row) > 1)

        if max_similarity > 0.8:
            recommendation = 'keep_best'  # Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¬Ø¯Ø§Ù‹ØŒ Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ø£ÙØ¶Ù„
        elif max_similarity > 0.5:
            recommendation = 'review_manual'  # Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù†ÙˆØ¹Ø§Ù‹ Ù…Ø§ØŒ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©
        else:
            recommendation = 'keep_all'  # Ù…Ø®ØªÙ„ÙØ©ØŒ Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„ÙƒÙ„

        return {
            'recommendation': recommendation,
            'sections': scored_sections,
            'similarity_matrix': similarity_matrix,
            'max_similarity': max_similarity
        }

    def _calculate_quality_score(self, section: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø¬ÙˆØ¯Ø© Ø§Ù„Ù‚Ø³Ù…"""
        score = 0.0
        text = section['text']

        # Ù†Ù‚Ø§Ø· Ù„Ù„Ø·ÙˆÙ„ (Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙØµÙ„ Ø£ÙØ¶Ù„)
        score += min(section['word_count'] / 100, 5.0)

        # Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª
        equations = len(re.findall(r'\$.*?\$|\\\[.*?\\\]', text))
        score += equations * 2.0

        # Ù†Ù‚Ø§Ø· Ù„Ù„ØªÙ†Ø¸ÙŠÙ… (Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ©)
        headers = len(re.findall(r'\*\*.*?\*\*', text))
        score += headers * 1.0

        # Ù†Ù‚Ø§Ø· Ù„Ù„Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª
        examples = len(re.findall(r'Ù…Ø«Ø§Ù„|ØªØ·Ø¨ÙŠÙ‚|Ø­Ø§Ù„Ø©', text, re.IGNORECASE))
        score += examples * 1.5

        # Ù†Ù‚Ø§Ø· Ù„Ù„ÙˆØ¶ÙˆØ­ (Ø¬Ù…Ù„ Ù‚ØµÙŠØ±Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©)
        sentences = text.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
        if 10 <= avg_sentence_length <= 25:  # Ø·ÙˆÙ„ Ù…Ø«Ø§Ù„ÙŠ
            score += 2.0

        return score
    
    def interactive_search(self):
        """ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ"""
        print("\nğŸ¯ ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ")
        print("Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©:")
        print("  search <Ù…ØµØ·Ù„Ø­>     - Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØµØ·Ù„Ø­")
        print("  similar <Ø¹ØªØ¨Ø©>      - Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª")
        print("  duplicates          - Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª")
        print("  structure           - ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡ÙŠÙƒÙ„")
        print("  pattern <Ù†Ù…Ø·>       - Ø§Ù„Ø¨Ø­Ø« Ø¨Ù†Ù…Ø· regex")
        print("  map <Ù…ØµØ·Ù„Ø­1,Ù…ØµØ·Ù„Ø­2> - Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª")
        print("  quit                - Ø®Ø±ÙˆØ¬")
        
        while True:
            try:
                command = input("\nğŸ” Ø£Ø¯Ø®Ù„ Ø§Ù„Ø£Ù…Ø±: ").strip()
                
                if command.lower() == 'quit':
                    break
                
                parts = command.split(' ', 1)
                cmd = parts[0].lower()
                arg = parts[1] if len(parts) > 1 else ""
                
                if cmd == 'search' and arg:
                    results = self.search_concept(arg)
                    self._display_search_results(results)
                
                elif cmd == 'similar':
                    threshold = float(arg) if arg else 0.7
                    results = self.find_similar_lines(threshold)
                    self._display_similarity_results(results)
                
                elif cmd == 'duplicates':
                    results = self.find_duplicates_exact()
                    self._display_duplicate_results(results)
                
                elif cmd == 'structure':
                    structure = self.analyze_section_structure()
                    self._display_structure_results(structure)
                
                elif cmd == 'pattern' and arg:
                    results = self.find_ideas_by_pattern(arg)
                    self._display_pattern_results(results)
                
                elif cmd == 'map' and arg:
                    terms = [t.strip() for t in arg.split(',')]
                    line_map = self.generate_line_map(terms)
                    self._display_line_map(line_map)
                
                else:
                    print("âŒ Ø£Ù…Ø± ØºÙŠØ± ØµØ­ÙŠØ­ Ø£Ùˆ Ù…ØµØ·Ù„Ø­ Ù…ÙÙ‚ÙˆØ¯")
            
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£: {e}")
    
    def _display_search_results(self, results: List[Dict]):
        """Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«"""
        if not results:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
            return
        
        print(f"\nğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(results)}):")
        for i, result in enumerate(results[:10]):  # Ø£ÙˆÙ„ 10 Ù†ØªØ§Ø¦Ø¬
            print(f"\n{i+1}. Ø§Ù„Ø³Ø·Ø± {result['line_number']}:")
            for context_line in result['context']:
                print(f"  {context_line}")
    
    def _display_similarity_results(self, results: List[Dict]):
        """Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ´Ø§Ø¨Ù‡"""
        if not results:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡Ø§Øª")
            return
        
        print(f"\nğŸ“‹ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª ({len(results)}):")
        for i, group in enumerate(results[:5]):  # Ø£ÙˆÙ„ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
            print(f"\n{i+1}. Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªØ´Ø§Ø¨Ù‡Ø© (Ø¯Ø±Ø¬Ø©: {group['similarity_score']:.2f}):")
            for line_num, content in group['lines']:
                print(f"  Ø§Ù„Ø³Ø·Ø± {line_num}: {content[:100]}...")
    
    def _display_duplicate_results(self, results: List[Dict]):
        """Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª"""
        if not results:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªÙƒØ±Ø§Ø±Ø§Øª")
            return
        
        print(f"\nğŸ“‹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ({len(results)}):")
        for i, dup in enumerate(results[:5]):  # Ø£ÙˆÙ„ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
            print(f"\n{i+1}. Ù…ÙƒØ±Ø± {dup['count']} Ù…Ø±Ø§Øª:")
            for line_num, content in dup['lines']:
                print(f"  Ø§Ù„Ø³Ø·Ø± {line_num}: {content[:100]}...")
    
    def _display_structure_results(self, structure: Dict):
        """Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡ÙŠÙƒÙ„"""
        print(f"\nğŸ“Š Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Øµ:")
        for section_type, items in structure.items():
            if items:
                print(f"\n{section_type} ({len(items)}):")
                for item in items[:5]:  # Ø£ÙˆÙ„ 5 Ø¹Ù†Ø§ØµØ±
                    print(f"  Ø§Ù„Ø³Ø·Ø± {item['line_number']}: {item['match']}")
    
    def _display_pattern_results(self, results: List[Dict]):
        """Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù†Ù…Ø·"""
        if not results:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬")
            return
        
        print(f"\nğŸ“‹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù…Ø· ({len(results)}):")
        for i, result in enumerate(results[:10]):
            print(f"{i+1}. Ø§Ù„Ø³Ø·Ø± {result['line_number']}: {result['match']}")
    
    def _display_line_map(self, line_map: Dict):
        """Ø¹Ø±Ø¶ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø£Ø³Ø·Ø±"""
        print(f"\nğŸ—ºï¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª:")
        for term, locations in line_map.items():
            print(f"\n'{term}' ({len(locations)} Ù…Ø±Ø©):")
            for loc in locations[:5]:  # Ø£ÙˆÙ„ 5 Ù…ÙˆØ§Ù‚Ø¹
                print(f"  Ø§Ù„Ø³Ø·Ø± {loc['line_number']}: {loc['content']}")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„ÙØ±Ø² Ø§Ù„Ù†ØµÙˆØµ")
    print("=" * 50)
    
    helper = AIAssistantHelper()
    
    # ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù
    file_path = input("ğŸ“ Ø£Ø¯Ø®Ù„ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù: ").strip()
    if not file_path:
        file_path = "riemann_research_ideas.md"  # Ø§ÙØªØ±Ø§Ø¶ÙŠ
    
    if helper.load_file(file_path):
        helper.interactive_search()
    else:
        print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù")

class QuickAssistant:
    """Ù…Ø³Ø§Ø¹Ø¯ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"""

    @staticmethod
    def quick_find_duplicates(file_path: str) -> str:
        """Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø¹ ØªÙ‚Ø±ÙŠØ± Ù…Ø®ØªØµØ±"""
        helper = AIAssistantHelper()
        if not helper.load_file(file_path):
            return "âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù"

        duplicates = helper.find_duplicates_exact()

        report = f"ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹:\n"
        report += f"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {len(duplicates)}\n\n"

        for i, dup in enumerate(duplicates[:10]):  # Ø£ÙˆÙ„ 10
            report += f"{i+1}. Ù…ÙƒØ±Ø± {dup['count']} Ù…Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: "
            line_numbers = [str(line[0]) for line in dup['lines']]
            report += ", ".join(line_numbers) + "\n"
            report += f"   Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {dup['lines'][0][1][:80]}...\n\n"

        return report

    @staticmethod
    def quick_search_multiple(file_path: str, terms: List[str]) -> str:
        """Ø¨Ø­Ø« Ø³Ø±ÙŠØ¹ Ø¹Ù† Ø¹Ø¯Ø© Ù…ØµØ·Ù„Ø­Ø§Øª"""
        helper = AIAssistantHelper()
        if not helper.load_file(file_path):
            return "âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù"

        report = f"ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¹Ø¯Ø¯:\n\n"

        for term in terms:
            results = helper.search_concept(term, context_lines=0)
            report += f"'{term}': {len(results)} Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„Ø£Ø³Ø·Ø± "
            if results:
                line_numbers = [str(r['line_number']) for r in results[:10]]
                report += ", ".join(line_numbers)
            else:
                report += "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬"
            report += "\n"

        return report

    @staticmethod
    def quick_similarity_check(file_path: str, threshold: float = 0.8) -> str:
        """ÙØ­Øµ Ø³Ø±ÙŠØ¹ Ù„Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø§Ù„Ø¹Ø§Ù„ÙŠØ©"""
        helper = AIAssistantHelper()
        if not helper.load_file(file_path):
            return "âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù"

        similar_groups = helper.find_similar_lines(threshold)

        report = f"ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª (Ø¹ØªØ¨Ø© {threshold}):\n"
        report += f"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©: {len(similar_groups)}\n\n"

        for i, group in enumerate(similar_groups[:5]):
            report += f"{i+1}. ØªØ´Ø§Ø¨Ù‡ {group['similarity_score']:.2f} ÙÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: "
            line_numbers = [str(line[0]) for line in group['lines']]
            report += ", ".join(line_numbers) + "\n"

        return report

def quick_help_commands():
    """Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    commands = {
        'find_dups': 'Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©',
        'find_similar': 'Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª',
        'search_terms': 'Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØµØ·Ù„Ø­Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©',
        'analyze_structure': 'ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Øµ',
        'line_map': 'Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ù…Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø£Ø³Ø·Ø±'
    }

    print("ğŸ¤– Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©:")
    for cmd, desc in commands.items():
        print(f"  {cmd}: {desc}")

def ai_assistant_workflow(file_path: str, task: str = "full_analysis"):
    """Ø³ÙŠØ± Ø¹Ù…Ù„ Ù…Ø®ØµØµ Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""

    print(f"ğŸ¤– Ø¨Ø¯Ø¡ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„: {task}")
    print("=" * 50)

    helper = AIAssistantHelper()
    if not helper.load_file(file_path):
        return

    if task == "full_analysis":
        # ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„
        print("1ï¸âƒ£ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª...")
        duplicates = helper.find_duplicates_exact()

        print("2ï¸âƒ£ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª...")
        similarities = helper.find_similar_lines(0.7)

        print("3ï¸âƒ£ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡ÙŠÙƒÙ„...")
        structure = helper.analyze_section_structure()

        print("4ï¸âƒ£ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©...")
        important_terms = ['Ù‡Ø§Ù…Ù„ØªÙˆÙ†ÙŠ', 'Ø·ÙŠÙ', 'ÙØ±Ø¶ÙŠØ© Ø±ÙŠÙ…Ø§Ù†', 'Ø¨Ø±Ù‡Ø§Ù†', 'Ù†Ø¸Ø±ÙŠØ©']
        line_map = helper.generate_line_map(important_terms)

        # ØªÙ‚Ø±ÙŠØ± Ù…ÙˆØ¬Ø²
        print(f"\nğŸ“Š Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙˆØ¬Ø²:")
        print(f"  Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {len(duplicates)} Ù…Ø¬Ù…ÙˆØ¹Ø©")
        print(f"  Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª: {len(similarities)} Ù…Ø¬Ù…ÙˆØ¹Ø©")
        print(f"  Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {len(structure['main_headers'])}")
        print(f"  Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª: {len(structure['equations'])}")

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = generate_ai_report(duplicates, similarities, structure, line_map)
        with open('ai_assistant_report.txt', 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: ai_assistant_report.txt")

    elif task == "quick_check":
        # ÙØ­Øµ Ø³Ø±ÙŠØ¹
        print("âš¡ ÙØ­Øµ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©...")

        duplicates = helper.find_duplicates_exact()
        if duplicates:
            print(f"âš ï¸  ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(duplicates)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø©")
            for i, dup in enumerate(duplicates[:3]):
                line_nums = [str(line[0]) for line in dup['lines']]
                print(f"  {i+1}. Ø§Ù„Ø£Ø³Ø·Ø±: {', '.join(line_nums)}")

        similarities = helper.find_similar_lines(0.8)
        if similarities:
            print(f"âš ï¸  ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(similarities)} ØªØ´Ø§Ø¨Ù‡ Ø¹Ø§Ù„ÙŠ")
            for i, sim in enumerate(similarities[:3]):
                line_nums = [str(line[0]) for line in sim['lines']]
                print(f"  {i+1}. Ø§Ù„Ø£Ø³Ø·Ø±: {', '.join(line_nums)} (ØªØ´Ø§Ø¨Ù‡: {sim['similarity_score']:.2f})")

def generate_ai_report(duplicates, similarities, structure, line_map):
    """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""

    report = "ğŸ¤– ØªÙ‚Ø±ÙŠØ± Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n"
    report += "=" * 50 + "\n\n"

    # Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
    report += "ğŸ“‹ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©:\n"
    if duplicates:
        for i, dup in enumerate(duplicates[:10]):
            line_nums = [str(line[0]) for line in dup['lines']]
            report += f"{i+1}. Ø§Ù„Ø£Ø³Ø·Ø± {', '.join(line_nums)}: {dup['lines'][0][1][:100]}...\n"
    else:
        report += "Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø©\n"

    report += "\n" + "="*30 + "\n\n"

    # Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª
    report += "ğŸ” Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø§Ù„Ø¹Ø§Ù„ÙŠØ©:\n"
    if similarities:
        for i, sim in enumerate(similarities[:10]):
            line_nums = [str(line[0]) for line in sim['lines']]
            report += f"{i+1}. Ø§Ù„Ø£Ø³Ø·Ø± {', '.join(line_nums)} (ØªØ´Ø§Ø¨Ù‡: {sim['similarity_score']:.2f})\n"
    else:
        report += "Ù„Ø§ ØªÙˆØ¬Ø¯ ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø¹Ø§Ù„ÙŠØ©\n"

    report += "\n" + "="*30 + "\n\n"

    # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª
    report += "ğŸ—ºï¸ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©:\n"
    for term, locations in line_map.items():
        if locations:
            line_nums = [str(loc['line_number']) for loc in locations]
            report += f"'{term}': Ø§Ù„Ø£Ø³Ø·Ø± {', '.join(line_nums[:10])}\n"

    return report

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        command = sys.argv[1]
        file_path = sys.argv[2] if len(sys.argv) > 2 else "riemann_research_ideas.md"

        if command == "quick_dups":
            print(QuickAssistant.quick_find_duplicates(file_path))
        elif command == "quick_similar":
            print(QuickAssistant.quick_similarity_check(file_path))
        elif command == "workflow":
            task = sys.argv[3] if len(sys.argv) > 3 else "full_analysis"
            ai_assistant_workflow(file_path, task)
        elif command == "help":
            quick_help_commands()
        else:
            print("Ø£ÙˆØ§Ù…Ø± Ù…ØªØ§Ø­Ø©: quick_dups, quick_similar, workflow, help")
    else:
        main()
